{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preprocessing - version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import linear_model, metrics, ensemble\n",
    "from sklearn import model_selection\n",
    "from xgboost import XGBClassifier\n",
    "import os\n",
    "\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(filename):\n",
    "    raw_data = pd.read_csv(filename)\n",
    "    data = raw_data.copy()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop features\n",
    "def select_same_resp_feature(X_train):\n",
    "    same_resp_feature = []\n",
    "    for feature in X_train.columns:\n",
    "        if len(np.unique(X_train[feature])) == 1:\n",
    "            same_resp_feature.append(feature)\n",
    "    print(len(same_resp_feature))\n",
    "    return same_resp_feature\n",
    "\n",
    "def select_allocation_flag(X_train):\n",
    "    allocation_flag_feature = []\n",
    "    for feature in X_train.columns:\n",
    "        if feature[:2] == 'PX' or feature[:2] == 'HX':\n",
    "            #print(feature)\n",
    "            allocation_flag_feature.append(feature)\n",
    "    other_alloc = ['PRCITFLG', 'PRWERNAL', 'PRHERNAL']\n",
    "    allocation_flag_feature = allocation_flag_feature + other_alloc\n",
    "    print(len(allocation_flag_feature))\n",
    "    return allocation_flag_feature\n",
    "\n",
    "# def select_low_resp_feature(X_train):\n",
    "#     for line in feature3:\n",
    "#     print(np.argmax(np.bincount(line)))\n",
    "\n",
    "# drop useless features together\n",
    "def drop_features(X_train):\n",
    "    same_resp_feature = select_same_resp_feature(X_train)\n",
    "    allocation_flag_feature = select_allocation_flag(X_train)\n",
    "    drop_features = list(set(same_resp_feature + allocation_flag_feature))\n",
    "    X_train.drop(drop_features, axis=1, inplace=True)\n",
    "    print(X_train.shape)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_preprocessing(filename):\n",
    "    # read in the train file\n",
    "    train_data = read_file(filename)\n",
    "    y_train = train_data['target']\n",
    "    X_train_raw = train_data.drop('target', axis=1)\n",
    "\n",
    "    # no dropping raw data -> np array\n",
    "    X_train_raw_arr = X_train_raw.values\n",
    "    y_train_arr = y_train.values\n",
    "    \n",
    "    # drop features -> np array\n",
    "    to_drop_features = select_same_resp_feature(X_train_raw)\n",
    "    X_train_drop = X_train_raw.drop(to_drop_features, axis=1)\n",
    "    X_train_drop_arr = X_train_drop.values\n",
    "    \n",
    "    return X_train_raw_arr, y_train_arr, X_train_drop_arr, to_drop_features\n",
    "\n",
    "def test_data_preprocessing(filename, to_drop_features):\n",
    "    test_data = read_file(filename)\n",
    "    X_test_raw = test_data\n",
    "    X_test_raw_arr = X_test_raw.values\n",
    "\n",
    "    X_test_drop = X_test_raw.drop(to_drop_features, axis=1)\n",
    "    X_test_drop_arr = X_test_drop.values\n",
    "    \n",
    "    return X_test_raw_arr, X_test_drop_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw pd.dataframe data\n",
    "train_path = \"train_2008.csv\"\n",
    "test_path = \"test_2008.csv\"\n",
    "train_data = read_file(train_path)\n",
    "test_data = read_file(test_path)\n",
    "\n",
    "y_train = train_data['target']\n",
    "X_train = train_data.drop('target', axis=1)\n",
    "X_test = test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "['HRMONTH', 'HRYEAR4', 'HUTYPEA', 'HUTYPC', 'HRINTSTA', 'PEAFNOW', 'PRPERTYP', 'PULKDK4', 'PULKDK5', 'PULKDK6', 'PULKPS4', 'PULKPS5', 'PULKPS6', 'HXPHONEO', 'PXAGE']\n"
     ]
    }
   ],
   "source": [
    "# drop same response features\n",
    "same_resp_feature = select_same_resp_feature(X_train)\n",
    "X_train.drop(same_resp_feature, axis=1, inplace=True)\n",
    "X_test.drop(same_resp_feature, axis=1, inplace=True)\n",
    "print(same_resp_feature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58\n"
     ]
    }
   ],
   "source": [
    "# map negative response to -1\n",
    "for feature in X_train.columns:\n",
    "    X_train[feature] = X_train[feature].apply(lambda x: -1 if x < 0 else x)\n",
    "    X_test[feature] = X_test[feature].apply(lambda x: -1 if x < 0 else x)\n",
    "\n",
    "response_rates = X_train[X_train >= 0].count() / len(X_train)\n",
    "mostly_blank_feats = []\n",
    "for itm in response_rates.items():\n",
    "    if itm[1] < 0.01:\n",
    "        mostly_blank_feats.append(itm[0])\n",
    "print(len(mostly_blank_feats))\n",
    "\n",
    "X_train.drop(mostly_blank_feats, axis=1, inplace=True)\n",
    "X_test.drop(mostly_blank_feats, axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # categorical features\n",
    "# categorical_features = ['HUFINAL','GEREG', 'HUBUS', 'PTDTRACE', 'PENATVTY', 'PUABSOT', 'PEIO1COW', \n",
    "#                      'HUFINAL', 'GESTCEN', 'GESTFIPS', #'PEIO1ICD', 'PEIO2ICD', \n",
    "#                      'PRCITSHP', 'PUDIS', 'PRABSREA', 'PRWKSTAT', 'HUPRSCNT', \n",
    "#                      'PERRP', 'GTCBSAST', 'PRMJOCGR', 'HRHTYPE', ]\n",
    "\n",
    "\n",
    "\n",
    "# # Now dummy these features\n",
    "# train_dummy_df = pd.DataFrame()\n",
    "# test_dummy_df = pd.DataFrame()\n",
    "\n",
    "# for feature in categorical_features:\n",
    "#     train_dummy_vars = pd.get_dummies(X_train[feature], prefix=feature)\n",
    "#     train_dummy_df = pd.concat([train_dummy_df, train_dummy_vars], axis=1)\n",
    "    \n",
    "#     test_dummy_vars = pd.get_dummies(X_test[feature], prefix=feature)\n",
    "#     test_dummy_df = pd.concat([test_dummy_df, test_dummy_vars], axis=1)\n",
    "# # Drop the original categorical variables\n",
    "# X_train.drop(categorical_features, axis=1, inplace=True)\n",
    "# X_test.drop(categorical_features, axis=1, inplace=True)\n",
    "\n",
    "# # Add dummy vars to the data\n",
    "# X_train = pd.concat([X_train, train_dummy_df], axis=1)\n",
    "# X_test = pd.concat([X_test, test_dummy_df], axis=1)\n",
    "\n",
    "# Now the train and test data have different numbers of features -> fix it!\n",
    "feats_to_add_to_train = [f for f in X_test.columns if f not in X_train.columns]\n",
    "feats_to_add_to_test = [f for f in X_train.columns if f not in X_test.columns]\n",
    "\n",
    "for feat in feats_to_add_to_train:\n",
    "    X_train[feat] = 0\n",
    "for feat in feats_to_add_to_test:\n",
    "    X_test[feat] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64667, 309)\n",
      "(64667,)\n",
      "(16000, 309)\n"
     ]
    }
   ],
   "source": [
    "X = X_train.values\n",
    "Y = y_train.values\n",
    "X_t = X_test.values\n",
    "print(X.shape)\n",
    "print(Y.shape)\n",
    "print(X_t.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc:  0.9064725967996514\n",
      "test auc:  0.7916698711831637\n"
     ]
    }
   ],
   "source": [
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=10)\n",
    "clf_xgb = XGBClassifier(max_depth=9, gamma=0.2, subsample=0.9, min_child_weight=3, n_estimators=50, objective='binary:logistic') \n",
    "clf_xgb.fit(X_train, y_train)\n",
    "y_train_pred = clf_xgb.predict_proba(X_train)[:, 1]\n",
    "y_test_pred = clf_xgb.predict_proba(X_test)[:, 1]\n",
    "\n",
    "train_auc = metrics.roc_auc_score(y_train, y_train_pred)\n",
    "print(\"train auc: \", train_auc)\n",
    "test_auc = metrics.roc_auc_score(y_test, y_test_pred)\n",
    "print(\"test auc: \", test_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "y_test_pred_prob = clf_xgb.predict_proba(X_t)[:, 1]\n",
    "pd.DataFrame(y_test_pred_prob).to_csv(\"xgb_test2008.csv\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_test = {'max_depth':range(5,13,2), 'gamma':[0, 0.1, 0.2, 0.3], 'min_child_weight':range(1,6,2)}\n",
    "estimator = XGBClassifier(subsample=0.8, objective='binary:logistic', n_estimators=50)\n",
    "gsearch = model_selection.GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc', cv=5)\n",
    "gsearch.fit(X, Y)\n",
    "print(\"best_params: \", gsearch1.best_params_)\n",
    "print(\"best_score: \", gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_tesdt_pred).to_csv(\"gbdt_test2008.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'columns'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-7343c8cd4111>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfeat\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategorical_feats\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'columns'"
     ]
    }
   ],
   "source": [
    "X_test = read_file(\"train_2008.csv\")\n",
    "X_test.drop(same_resp_feature, axis=1, inplace=True)\n",
    "for feat in X_test.columns:\n",
    "    X_test[feat] = X_test[feat].apply(lambda x: -1 if x < 0 else x)\n",
    "for feat in categorical_feats:\n",
    "    X_test = pd.get_dummies(X_test, columns=[feat])\n",
    "print(X.shape)\n",
    "print(X_test.values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data for random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "(64667, 382)\n",
      "(64667,)\n",
      "(64667, 367)\n",
      "(16000, 382)\n",
      "(16000, 367)\n"
     ]
    }
   ],
   "source": [
    "train_path = \"train_2008.csv\"\n",
    "test_path = \"test_2008.csv\"\n",
    "X_train_raw_arr, y_train_arr, X_train_drop_arr, to_drop_features = train_data_preprocessing(train_path)\n",
    "X_test_raw_arr, X_test_drop_arr = test_data_preprocessing(test_path, to_drop_features)\n",
    "\n",
    "print(X_train_raw_arr.shape)\n",
    "print(y_train_arr.shape)\n",
    "print(X_train_drop_arr.shape)\n",
    "print(X_test_raw_arr.shape)\n",
    "print(X_test_drop_arr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_test(X_train_arr, y_train_arr, num_folds, C, X_test_arr, clf):\n",
    "    clf.fit(X_train_arr, y_train_arr)\n",
    "    y_train_pred_prob = clf.predict_proba(X_train_arr)[:, 1]\n",
    "    train_auc = metrics.roc_auc_score(y_train_arr, y_train_pred_prob)\n",
    "    print(\"train auc: \", train_auc)\n",
    "    y_test_pred_prob = clf.predict_proba(X_test_arr)[:, 1]\n",
    "    return clf, train_auc, y_test_pred_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate auc score for cross_validation\n",
    "def cross_validation(num_folds, X_train_arr, clf):\n",
    "    kf = model_selection.KFold(n_splits=num_folds)\n",
    "    auc = []\n",
    "    for train_index, test_index in kf.split(X_train_arr):\n",
    "        print(\"1---\")\n",
    "        X_train_cv, X_test_cv = X_train_arr[train_index], X_train_arr[test_index]\n",
    "        y_train_cv, y_test_cv = y_train_arr[train_index], y_train_arr[test_index]\n",
    "        clf.fit(X_train_cv,y_train_cv)\n",
    "        y_pred_prob = clf.predict_proba(X_test_cv)[:, 1]\n",
    "        auc.append(metrics.roc_auc_score(y_test_cv, y_pred_prob))\n",
    "        print(\"2---\")\n",
    "    return auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# logistic regression\n",
    "def logistic_reg(X_train_arr, num_folds, C):\n",
    "    clf = linear_model.LogisticRegression(penalty='l1', C=C, solver='liblinear')\n",
    "    #clf = linear_model.LogisticRegression()\n",
    "    #auc = cross_validation(num_folds, X_train_arr, clf)\n",
    "    #return auc, clf\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data for logistic regression\n",
    "\n",
    "# tune the hyperparameter C has little influence\n",
    "# current C =  0.3 , auc =  [0.7663607035376859, 0.7721732996821118, 0.7687268216387099]\n",
    "# current C =  0.475 , auc =  [0.7662418721269123, 0.7712735791351473, 0.768680422577126]\n",
    "# current C =  0.6499999999999999 , auc =  [0.7661768150447318, 0.7713118201969408, 0.7683741910137993]\n",
    "# current C =  0.825 , auc =  [0.7661509304142629, 0.7711936392253098, 0.7682658368117301]\n",
    "# current C =  1.0 , auc =  [0.766072324965325, 0.7707366180248577, 0.7682738111247072]\n",
    "\n",
    "# X_train_raw_arr, y_train_raw_arr, X_train_drop_arr = data_preprocessing(train_path, to_drop_features)\n",
    "# X_test_raw_arr, y_test_raw_arr, X_test_drop_arr = data_preprocessing(test_path, to_drop_features)\n",
    "# num_folds = 3\n",
    "# C_para = np.linspace(0.3, 1, 5)\n",
    "# for C in C_para:\n",
    "#     auc = logistic_reg(X_train_raw_arr, num_folds, C)\n",
    "#     print(\"current C = \", C, \", auc = \", auc)\n",
    "\n",
    "# ??? how can I ensemble LR models? will ensemble here improves the performance?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train auc:  0.7742189701790714\n",
      "0.7742189701790714\n",
      "0.7719477543415707\n",
      "[[ 1.06529611e-06  1.04506420e-02  1.73326711e-03  8.31401467e-02\n",
      "   3.53121147e-01  1.16660750e-01  3.59783834e-01 -4.63037852e-02\n",
      "  -4.40578302e-03 -1.93829637e-02  1.16503131e-08  1.04606198e-02\n",
      "  -2.29482596e-02  1.17185565e-02 -1.03245149e-01 -2.16063165e-04\n",
      "  -3.72117647e-02  6.98180804e-06  1.61678073e-01 -2.37551192e-03\n",
      "  -5.21249193e-02 -2.56715484e-01  1.21445642e-02  3.93426980e-03\n",
      "   2.09660268e-07 -2.97096563e-04  2.42630526e-03 -1.13974347e-03\n",
      "  -5.73297622e-02 -2.45993565e-02 -1.87942729e-04 -2.24134320e-02\n",
      "   7.99248596e-02 -2.72194955e-02  3.96643124e-01 -1.92581188e-01\n",
      "  -5.26471984e-02 -2.03686157e-01  7.52618135e-02 -9.17889663e-02\n",
      "   2.66786055e-02 -1.19624563e-01 -8.95974318e-03 -1.24795072e-01\n",
      "   4.00329707e-02  1.08988456e-01  1.97889454e-01 -4.53304637e-01\n",
      "   1.56011740e-01 -3.26836248e-03  3.19138353e-04  3.97854855e-04\n",
      "   3.00764489e-01  2.92101968e-02  3.38797897e-02  7.58800595e-02\n",
      "   1.37547175e-01  8.01672622e-02 -1.75885857e-05  0.00000000e+00\n",
      "   3.21144550e-02  0.00000000e+00  3.51172123e-02  6.90875353e-02\n",
      "  -5.37822266e-03  0.00000000e+00  7.47993298e-02 -6.70761866e-02\n",
      "  -1.60134176e-03 -7.03259030e-02  3.67835017e-02 -1.11642294e-01\n",
      "  -2.71055279e-03  6.19219288e-03  0.00000000e+00  5.61563819e-03\n",
      "  -1.86256586e-01 -1.38431510e-02 -2.19876833e-02 -7.59173364e-03\n",
      "   1.36997110e-03  2.52301581e-03  0.00000000e+00 -8.76358838e-04\n",
      "  -5.96043849e-04 -1.41992111e-03 -1.13365443e-03  8.80675246e-02\n",
      "  -6.55563724e-05  1.91351488e-03  8.75331554e-03  2.63981662e-02\n",
      "  -6.78441804e-02 -4.62885461e-03  5.43023260e-02 -1.98346131e-02\n",
      "  -1.60486912e-01  9.46449140e-02 -2.70520890e-02 -5.69937382e-04\n",
      "  -8.14867459e-05 -5.96021850e-03 -2.18397241e-03 -5.56492148e-02\n",
      "   2.57506251e-02  0.00000000e+00  2.61480571e-02  0.00000000e+00\n",
      "   0.00000000e+00  7.47681830e-02  0.00000000e+00 -2.67388369e-03\n",
      "   0.00000000e+00 -2.44262861e-01 -3.01915874e-02  0.00000000e+00\n",
      "  -1.45819459e-01 -5.39358995e-02  1.14212832e-03  2.24378645e-01\n",
      "   8.20895949e-03  1.39932668e-01  0.00000000e+00  1.62517195e-02\n",
      "  -3.85653454e-03  0.00000000e+00  2.28179630e-02  1.16708272e-02\n",
      "  -2.69730031e-02  1.89261778e-02  2.19038897e-01 -6.10316456e-04\n",
      "  -1.26892423e-04  0.00000000e+00  1.48296971e-02 -1.18964605e-01\n",
      "   7.52228292e-03 -2.35612054e-01 -1.71327205e-02 -3.63819976e-03\n",
      "   4.65377816e-02  2.86618085e-02  0.00000000e+00  5.76623382e-04\n",
      "   9.31433527e-02  3.14322056e-01  4.63601937e-02 -1.83972325e-01\n",
      "  -3.60190497e-02 -1.51563414e-02 -1.61331358e-02  8.41042757e-02\n",
      "  -8.21683057e-02 -1.25182098e-01  1.65942838e-01 -7.20771172e-02\n",
      "   0.00000000e+00  0.00000000e+00  0.00000000e+00  1.43516410e-01\n",
      "  -6.06632034e-02  7.63323149e-02 -6.28519253e-02  9.45021323e-05\n",
      "  -1.02145475e-04  3.71344468e-06  0.00000000e+00  4.82351053e-03\n",
      "  -6.80663028e-07  2.82313282e-01  0.00000000e+00  3.54919415e-03\n",
      "   0.00000000e+00  3.76861165e-02  8.04160120e-03  6.03610377e-02\n",
      "   8.36462788e-02  0.00000000e+00 -8.70070710e-02 -2.23174877e-02\n",
      "   0.00000000e+00 -2.27418527e-01  1.38629451e-01 -9.17022427e-09\n",
      "   9.28855248e-10 -1.92633618e-10 -3.95282039e-09  4.56994672e-10\n",
      "  -3.10046534e-02 -1.43215896e-02 -3.64530454e-02  0.00000000e+00\n",
      "   2.92081940e-03  7.22097417e-03 -3.03581683e-03 -8.72505561e-03\n",
      "  -1.14627707e-02  1.46341315e-03 -1.28108844e-02  3.12381159e-02\n",
      "  -2.52430181e-02 -2.12540430e-03  6.98815915e-03  1.54145393e-02\n",
      "  -3.45928395e-03  1.87098021e-02 -5.36785789e-02 -3.44315033e-02\n",
      "   2.70443907e-02 -2.76598912e-04  1.50164329e-02  3.27557574e-02\n",
      "  -2.92179881e-02 -2.40991279e-02 -1.70667495e-03  4.25860806e-02\n",
      "  -3.52258857e-03 -8.48554951e-03  1.09008561e-03 -6.49183102e-03\n",
      "  -2.98336351e-03  2.34698208e-02  4.03508969e-03  0.00000000e+00\n",
      "  -1.00147933e-02 -1.56065709e-01  1.73316419e-02  7.20750570e-03\n",
      "  -5.61974302e-02 -7.05685793e-03 -1.41249385e-03  3.80814946e-03\n",
      "   3.10006932e-03 -4.57693693e-02  9.20667178e-02 -3.69820185e-01\n",
      "   2.74886552e-02 -4.13346923e-02  6.13603937e-02 -2.87162110e-03\n",
      "   1.10539529e-01  1.15008552e-02  4.18847412e-03 -9.97106844e-04\n",
      "  -2.52698180e-03 -3.08316437e-02 -3.06424340e-02  4.10239410e-02\n",
      "   1.19086099e-02 -4.36563181e-02  3.61601977e-02 -1.32920893e-01\n",
      "   9.30062425e-03 -3.37873106e-02 -1.14561455e-02 -2.65919527e-03\n",
      "   2.00357989e-03  3.90168495e-03  6.90889422e-03  5.08217537e-02\n",
      "   5.35754928e-03  2.99520468e-02 -9.68635558e-03 -1.44312015e-01\n",
      "  -5.03617471e-02  1.44891186e-07  1.67035056e-01 -8.28791247e-02\n",
      "   4.59269395e-02 -1.55704810e-01 -2.91508889e-01 -3.14961985e-01\n",
      "  -3.24983245e-01 -9.44257719e-03  4.21103225e-03 -9.52027431e-04\n",
      "   2.15747041e-03 -2.86962719e-02  1.92440400e-02 -1.72180274e-09\n",
      "   2.71416517e-05  1.09269448e-04  6.79817400e-02 -2.09098384e-02\n",
      "  -1.50261470e-02 -2.47200439e-03  4.20721751e-02  0.00000000e+00\n",
      "   3.65509098e-02 -1.32815097e-01  8.21874362e-02  4.53490122e-03\n",
      "   1.81809251e-04  1.70285645e-03 -3.63201955e-03 -2.07320664e-05\n",
      "  -3.87912915e-02  0.00000000e+00 -2.04851250e-01 -7.38216053e-02\n",
      "  -7.09342269e-02 -3.89892549e-01 -2.20204469e-02  4.43185291e-03\n",
      "  -7.18010953e-03 -6.65808993e-04 -1.21359944e-05  7.08646173e-03\n",
      "  -5.72412660e-03]]\n"
     ]
    }
   ],
   "source": [
    "# run logistic regression\n",
    "num_folds = 5\n",
    "C = 0.3\n",
    "test_size = 0.3\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(X, Y, test_size=test_size, random_state=10)\n",
    "clf_lr = logistic_reg(X_train, num_folds, C)\n",
    "clf, train_auc, y_test_pred_prob = run_test(X_train, y_train, num_folds, C, X_test, clf_lr)\n",
    "print(train_auc)\n",
    "test_auc = metrics.roc_auc_score(y_test, y_test_pred_prob)\n",
    "print(test_auc)\n",
    "print(clf.coef_)\n",
    "\n",
    "\n",
    "#pd.DataFrame(y_test_pred_prob).to_csv(\"lr_test2008.csv\")\n",
    "#print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "print(len(clf.coef_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gbdt\n",
    "def gbdt(X_train_arr, num_folds):\n",
    "    #clf_gbdt = ensemble.GradientBoostingClassifier(random_state=10)\n",
    "    clf_gbdt = ensemble.GradientBoostingClassifier(n_estimators=50, random_state=10, min_samples_leaf=20) \n",
    "    #clf_gbdt = ensemble.GradientBoostingClassifier(learning_rate=0.5, min_samples_split=300, min_samples_leaf=20,max_depth=5,max_features='sqrt', subsample=0.8,random_state=10)\n",
    "    #auc = cross_validation(num_folds, X_train_arr, clf_gbdt)\n",
    "    #print(\"auc = \", auc)\n",
    "    return clf_gbdt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "train auc:  0.8254147729384751\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# run gbdt\n",
    "X_train_raw_arr, y_train_arr, X_train_drop_arr, to_drop_features = train_data_preprocessing(train_path)\n",
    "X_test_raw_arr, X_test_drop_arr = test_data_preprocessing(test_path, to_drop_features)\n",
    "num_folds = 3\n",
    "clf_gbdt = gbdt(X_train_raw_arr, num_folds)\n",
    "train_auc, y_test_pred_prob = run_test(X_train_raw_arr, y_train_arr, num_folds, C, X_test_raw_arr, clf_gbdt)\n",
    "\n",
    "\n",
    "# pd.DataFrame(y_test_pred_prob).to_csv(\"gbdt_raw_test2008.csv\")\n",
    "# print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "train auc:  0.7929522993012791\n",
      "done!\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-f76d3e82eb55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m )\n\u001b[1;32m     10\u001b[0m \u001b[0mgsearch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_selection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparam_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'roc_auc'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0mgsearch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_raw_arr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_arr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_params: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"best_score: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgsearch1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    720\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 722\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    723\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    724\u001b[0m         \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresults_container\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1191\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    709\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 711\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    712\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m                 \u001b[0mall_candidate_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcandidate_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    918\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 920\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    921\u001b[0m                 \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    757\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    758\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 759\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    760\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    714\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    715\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 716\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    717\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/externals/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    548\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 549\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    550\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    223\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    224\u001b[0m             return [func(*args, **kwargs)\n\u001b[0;32m--> 225\u001b[0;31m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[1;32m    226\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    526\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 528\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    529\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[1;32m   1463\u001b[0m         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,\n\u001b[1;32m   1464\u001b[0m                                     \u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1465\u001b[0;31m                                     begin_at_stage, monitor, X_idx_sorted)\n\u001b[0m\u001b[1;32m   1466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1467\u001b[0m         \u001b[0;31m# change shape of arrays after fit (early-stopping or additional ests)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stages\u001b[0;34m(self, X, y, y_pred, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1527\u001b[0m             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,\n\u001b[1;32m   1528\u001b[0m                                      \u001b[0msample_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m                                      X_csc, X_csr)\n\u001b[0m\u001b[1;32m   1530\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1531\u001b[0m             \u001b[0;31m# track deviance (= loss)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/ensemble/gradient_boosting.py\u001b[0m in \u001b[0;36m_fit_stage\u001b[0;34m(self, i, X, y, y_pred, sample_weight, sample_mask, random_state, X_idx_sorted, X_csc, X_csr)\u001b[0m\n\u001b[1;32m   1192\u001b[0m             \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX_csr\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1193\u001b[0m             tree.fit(X, residual, sample_weight=sample_weight,\n\u001b[0;32m-> 1194\u001b[0;31m                      check_input=False, X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m             \u001b[0;31m# update tree leaves\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m   1140\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1141\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1142\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m   1143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/xgboost/lib/python3.6/site-packages/sklearn/tree/tree.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    364\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    365\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 366\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    367\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    368\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "X_train_raw_arr, y_train_arr, X_train_drop_arr, to_drop_features = train_data_preprocessing(train_path)\n",
    "X_test_raw_arr, X_test_drop_arr = test_data_preprocessing(test_path, to_drop_features)\n",
    "param_test = {'min_samples_split':range(200,900,100), 'min_samples_leaf': range(30, 70, 10), 'max_depth': range(5, 10, 1)}\n",
    "estimator = ensemble.GradientBoostingClassifier(\n",
    "    learning_rate=0.8,\n",
    "    #max_features='sqrt',\n",
    "    random_state=10,\n",
    "    n_estimators=50\n",
    ")\n",
    "gsearch = model_selection.GridSearchCV(estimator, param_grid = param_test, scoring='roc_auc', cv=5)\n",
    "gsearch.fit(X_train_raw_arr, y_train_arr)\n",
    "print(\"best_params: \", gsearch1.best_params_)\n",
    "print(\"best_score: \", gsearch1.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15\n",
      "train auc:  0.7818926933969366\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "X_train_raw_arr, y_train_arr, X_train_drop_arr, to_drop_features = train_data_preprocessing(train_path)\n",
    "X_test_raw_arr, X_test_drop_arr = test_data_preprocessing(test_path, to_drop_features)\n",
    "num_folds = 3\n",
    "clf_gbdt = gbdt(X_train_raw_arr, num_folds)\n",
    "train_auc, y_test_pred_prob = run_test(X_train_raw_arr, y_train_arr, num_folds, C, X_test_raw_arr, clf_gbdt)\n",
    "pd.DataFrame(y_test_pred_prob).to_csv(\"gbdt_raw_test2008.csv\")\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
